\documentclass[12pt]{article} % JASA requires 12 pt font for manuscripts
%\usepackage{JASA_manu}        % For JASA manuscript formatting

% for citations
\usepackage[authoryear]{natbib} % natbib required for JASA
\usepackage[colorlinks=true, citecolor=blue, linkcolor=blue]{hyperref}
\newcommand{\citetapos}[1]{\citeauthor{#1}{\textcolor{blue}{'s}} }

%\definecolor{Blue}{rgb}{0,0,0.5}

\usepackage{amsthm}

% for figures
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfig}
\captionsetup[subfloat]{font=normalsize}
%\usepackage{subcaption}
\graphicspath{{figures/}}
\newcommand{\hh}[1]{{\color{orange} #1}}
\newcommand{\al}[1]{{\color{red} #1}}

% color in tables
\usepackage{rotating}
\usepackage{color}
\usepackage{colorbl}

% help with editing and coauthoring
\usepackage{todonotes}

% title formatting
\usepackage[compact,small]{titlesec}
% page formatting
\usepackage[margin = 1in]{geometry}
\usepackage[parfill]{parskip}

% line spacing
\usepackage{setspace}
\doublespace

% For math typsetting
\usepackage{bm}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}

\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{algorithm}[theorem]{Algorithm}

% A few commands to make typing less tedious
\newcommand{\inv}{\ensuremath{^{-1}}}
\newcommand{\ginv}{\ensuremath{^{-}}}
\newcommand{\trans}{\ensuremath{^\prime}}
\newcommand{\E}{\ensuremath{\mathrm{E}}}
\newcommand{\var}{\ensuremath{\mathrm{Var}}}
\newcommand{\cov}{\ensuremath{\mathrm{Cov}}}
\DeclareMathOperator{\tr}{Trace}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}

In this document I am going to try to clarify the optimization process, and by doing so, I hope that I answer your question about why the amount of confounding is not being reduced more. I thought that starting this as a separate note and then integrating it into the paper were necessary would be easier to work with than trying to make notes in the paper at this point.

\paragraph{Our optimization problem.} We propose solving the following optimization problem in an effort to minimize the amount of confounding in the problem for a specified dimension, $s$
%
\begin{equation}\label{eq:minimize}
\bm{W}^* = \argmin_{W \in \mathbb{R}^{\ell \times s} } 
\tr\left( \left(\bm{W\trans B W} \right)\inv \left(\bm{W\trans A W}\right) \right)
%\displaystyle{\sum_i} \frac{\bm{v_i}\trans \bm{W} \bm{A} \bm{W}\trans \bm{v_i}}
%		{\bm{v_i}\trans \bm{W} \bm{B} \bm{W}\trans \bm{v_i}}
\end{equation}
%
Mathematically, this problem is solved using the generalized eigenvalue decomposition
\begin{equation}\label{eq:geigen}
	\bm{Aw}_k = \gamma_k \bm{Bw}_k
\end{equation}
where $\gamma_k$ and $\bm{w}_k$ are the $k$-th smallest eigenvalues and eigenvectors, respectively; thus, $\bm{W}^*$ consists of the eigenvectors associated with the $s$ smallest eigenvalues. Computationally we solve this problem through simultaneous diagonalization, which requires $\bm{W}$ to satisfy $\bm{W\trans B W} = \bm{I}$ (this is sometimes called $\bm{B}$-orthgonality); thus, \eqref{eq:minimize} can be rewritten as
%
\begin{equation}
\bm{W}^* = \argmin_{ 
\begin{scriptsize}
	\begin{cases}
      \bm{W} \in \mathbb{R}^{\ell \times s} \\
      \bm{W\trans B W} = \bm{I}
	\end{cases}
\end{scriptsize}
	} 
\tr\left( \bm{W\trans A W} \right) 
\end{equation}
%
So simultaneous diagonalization (Algorithm 1 in the paper) first ensures that the constraint is satisfied, and then diagonalizes $\bm{A}$ which gives us our solution (those details are pretty good in the paper). Note that through simultaneous diagonalization we also get $\bm{W}\trans \bm{W} = \bm{WW}\trans = \bm{I}$.


Note that this optimization problem is essentially optimizing
\[
	\sum_i \frac{\bm{w_i\trans A w_i}}{\bm{w_i\trans B w_i}}
\]

\paragraph{Why doesn't FC shrink faster?\\}
To answer this, we first need to think about the properties of the trace. Recall that the trace is invariant to cyclic permutations; thus, 
%\[
%	\tr(\bm{W\trans A W}) = \tr(\bm{A W W\trans}) = \tr(\bm{A})
%\] 
Recall that, for a matrix $\bm{M}$, $\tr (\bm{M}) = \sum_i \lambda_i$, where $\lambda_1, \ldots, \lambda_n$ are the ordered eigenvalues of $\bm{M}$. If $\bm{M}$ is not full rank, but is positive semidefinite, we can write $\tr (\bm{M}) = \lambda_1 + \cdots + \lambda_r + 0 + \cdots + 0$. 

So in our problem we consider the value of the trace in an $s$-dimensional space, which is written as (note that simplification follows from the constraint)
\[
J_i(s)  
	= tr\left( \left(\bm{W\trans B W} \right)\inv \left(\bm{W\trans A W}\right) \right) 
	= tr\left(\bm{W\trans A W}\right)
 	= \sum_{i \in \{r, r-1, \ldots, r-(s+1) \}} \lambda_i
\]
The minimum is found by selecting the $s$ smallest eigenvalues, which will have value equal to the sum. In the above notation we use $r$ to denote the rank of $\bm{A}$, and assume that the eigenvalues are ordered so that, given $s$, we select the $r$th through $r-(s+1)$th eigenvalues. This means that if we select $s$ to be the rank of $\bm{A}$, then the trace will attain its original value, with the only produce being the elimination of redundant information rather than progress toward reducing the objective function.

I did find an error in my computation of FC for Figure 6. I computed the resulting values of FC by taking the mean of the diagonal entries; however, we are really working with the trace rather than FC, which I defined the be the average, \todo{Maybe we have to redefine?} so I was changing the denominator which really makes the quantities incomparable. One thing we could do is always use the length of the initial vector of random effects to compare this ``average'' amount of confounding, or we could just plot the values of the trace.

I am still not writing this very clearly, but will keep trying.

%We are minimizing the trace given a choice of the dimension of the resulting subspace. If we choose $s=1$, then the minimum is the smallest eigenvalue. If we choose $s=2$, then the minimum is the sum of the two smallest eigenvalues. This goes on and yields that if we choose $s = \rank(\bm{M})$

\todo[inline]{Essentially, I believe FC isn't shrinking faster because \\
(1) if we don't reduce $s$ from the rank we don't reduce FC at all\\ 
(2) The radon example, and the simulations from it, have many highly confounded groups. This means that the high FC isn't just a few highly confounded groups that inflate the calculation of FC, which would be shrunken quickly using our optimization, but has an overall high level of confounding in many groups.}

\end{document}